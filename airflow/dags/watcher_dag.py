from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.models import Variable
from airflow.utils.dates import days_ago
from pathlib import Path
import os
import re

# === CONFIGURATION ===
# Dossier Ã  surveiller (Raw data ou Prepro, selon votre besoin)
# Ici on surveille les donnÃ©es brutes pour dÃ©clencher tout le pipeline si besoin
WATCH_DIR = Path("/opt/airflow/data") 
TARGET_DAG_ID = "on_demand_prediction" # Le DAG Ã  dÃ©clencher

default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
}

def scan_for_new_batches(**context):
    print(f"ğŸ” Scan du dossier : {WATCH_DIR}")
    
    if not WATCH_DIR.exists():
        print("âš ï¸ Dossier introuvable.")
        return []

    # 1. RÃ©cupÃ©rer tous les fichiers images
    files = [f.name for f in WATCH_DIR.glob("*") if f.suffix.lower() in ['.jpg', '.png', '.jpeg']]
    
    # 2. Extraire les prÃ©fixes uniques (ex: s8, s9, s10)
    # On cherche ce qui est avant le premier underscore : "s8_0001.jpg" -> "s8"
    current_prefixes = set()
    for f in files:
        match = re.match(r"^([a-zA-Z0-9]+)_", f)
        if match:
            current_prefixes.add(match.group(1))
            
    print(f"ğŸ“‚ Lots trouvÃ©s sur le disque : {current_prefixes}")

    # 3. RÃ©cupÃ©rer l'historique des lots dÃ©jÃ  traitÃ©s (depuis les Variables Airflow)
    # La variable s'appellera 'processed_batches_list'
    # On stocke Ã§a sous forme de liste sÃ©parÃ©e par des virgules
    processed_str = Variable.get("processed_batches_list", default_var="")
    processed_prefixes = set(processed_str.split(",")) if processed_str else set()

    # 4. Identifier les nouveaux
    new_prefixes = list(current_prefixes - processed_prefixes)
    
    if not new_prefixes:
        print("âœ… Rien de nouveau.")
        return None # Rien Ã  faire

    print(f"ğŸš€ Nouveaux lots dÃ©tectÃ©s : {new_prefixes}")
    
    # 5. Mettre Ã  jour la variable TOUT DE SUITE pour ne pas les relancer au prochain scan
    # On ajoute les nouveaux Ã  l'existant
    updated_processed = processed_prefixes.union(new_prefixes)
    Variable.set("processed_batches_list", ",".join(updated_processed))

    # 6. Retourner la liste pour l'Ã©tape suivante
    return new_prefixes

def trigger_target_dags(ti):
    # RÃ©cupÃ©rer la liste des nouveaux prÃ©fixes depuis la tÃ¢che prÃ©cÃ©dente (XCom)
    new_prefixes = ti.xcom_pull(task_ids='scan_files')
    
    if not new_prefixes:
        return

    from airflow.api.common.experimental.trigger_dag import trigger_dag
    
    # Pour chaque nouveau prÃ©fixe, on lance le DAG de prÃ©diction
    for prefix in new_prefixes:
        print(f"âš¡ DÃ©clenchement du DAG {TARGET_DAG_ID} pour le lot {prefix}...")
        try:
            trigger_dag(
                dag_id=TARGET_DAG_ID,
                conf={"prefix": prefix}, # On passe le paramÃ¨tre !
                replace_microseconds=False,
            )
        except Exception as e:
            print(f"âŒ Erreur lors du dÃ©clenchement pour {prefix}: {e}")

with DAG('file_watcher_sensor', 
         default_args=default_args, 
         schedule_interval='*/5 * * * *', # Scan toutes les 5 minutes
         catchup=False) as dag:

    # Ã‰tape 1 : Scanner et mettre Ã  jour la mÃ©moire
    t_scan = PythonOperator(
        task_id='scan_files',
        python_callable=scan_for_new_batches
    )

    # Ã‰tape 2 : DÃ©clencher les DAGs correspondants
    t_trigger = PythonOperator(
        task_id='trigger_predictions',
        python_callable=trigger_target_dags
    )

    t_scan >> t_trigger